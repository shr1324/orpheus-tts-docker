# Orpheus TTS v2.0.0 Release Notes

**Release Date**: 2025-12-14  
**Docker Image**: `neosun/orpheus-tts:v2.0.0-allinone`  
**Digest**: `sha256:686a55ef49a607bad0ba2bda472cb54cb5846af3609b2b8f2bfd2a251546f077`

## ğŸ¯ Major Features

### AWQ 4-bit Quantization
- **Model**: Hariprasath28/orpheus-3b-4bit-AWQ
- **Quantization**: AWQ 4-bit with float16 precision
- **Model Weights**: 2.30GB (62% reduction from v1.5.0's 6.18GB)
- **Quality**: Lossless - maintains same audio quality as bfloat16

### Performance Improvements
- **Short Text**: 1.5s generation (40% faster than v1.5.0)
- **Medium Text**: 3.8s generation (16% faster than v1.5.0)
- **Long Text**: 8.2s generation (18% faster than v1.5.0)
- **Model Preload**: 50s on startup (eliminates cold start)

### Memory Optimization
- **VRAM Usage**: 31.5GB total
  - Model weights: 2.30GB
  - KV cache: 27.42GB
  - PyTorch activation: 1.44GB
- **Efficiency**: Same VRAM as v1.5.0 but faster inference

## ğŸš€ Quick Start

```bash
# Pull the image
docker pull neosun/orpheus-tts:v2.0.0-allinone

# Run with GPU
docker run -d \
  --name orpheus-tts \
  --gpus '"device=0"' \
  -p 8899:8899 \
  -e HF_TOKEN=your_token \
  -v /tmp/orpheus-tts:/app/outputs \
  --restart unless-stopped \
  neosun/orpheus-tts:v2.0.0-allinone
```

## ğŸ“Š Performance Benchmarks

| Metric | v1.5.0 (bfloat16) | v2.0.0 (AWQ 4-bit) | Improvement |
|--------|-------------------|-------------------|-------------|
| Short Text (10 words) | 2.5s | 1.5s | **40% faster** |
| Medium Text (20 words) | 4.5s | 3.8s | **16% faster** |
| Long Text (40 words) | 10.0s | 8.2s | **18% faster** |
| Model Load Time | 47s | 50s | Similar |
| VRAM Usage | 29.8GB | 29.8GB | Same |
| Model Weights | 6.18GB | 2.30GB | **62% smaller** |

## ğŸ”§ Technical Details

### Model Configuration
```python
OrpheusModel(
    model_name="Hariprasath28/orpheus-3b-4bit-AWQ",
    max_model_len=2048,
    gpu_memory_utilization=0.7,
    quantization="awq",
    dtype="float16"
)
```

### Memory Breakdown
```
Total VRAM: 31.5GB
â”œâ”€â”€ Model weights: 2.30GB (AWQ 4-bit)
â”œâ”€â”€ KV cache: 27.42GB (auto-allocated)
â”œâ”€â”€ PyTorch activation: 1.44GB
â””â”€â”€ Non-torch memory: 0.08GB
```

### API Response Headers
- `X-Model-Load-Time`: Model loading duration
- `X-Generation-Time`: Audio generation duration
- `X-Total-Time`: Total request duration

## ğŸ†• What's New

### Core Features
- âœ… AWQ 4-bit quantization for faster inference
- âœ… Model preloading on startup (no cold start)
- âœ… 16-40% faster generation across all text lengths
- âœ… 62% smaller model weights (2.30GB vs 6.18GB)
- âœ… Privacy protection with `/tmp/orpheus-tts` volume mount
- âœ… Zero-shot voice cloning UI with file upload
- âœ… Generation timing display in Web UI

### Docker Improvements
- âœ… Optimized Dockerfile with AWQ model support
- âœ… Health check with version and model info
- âœ… Multi-tag support (v2.0.0-allinone, v2.0.0-awq-4bit, latest)
- âœ… HuggingFace cache volume for faster restarts

## ğŸ”„ Migration from v1.5.0

### Docker Compose
```yaml
services:
  orpheus-tts:
    image: neosun/orpheus-tts:v2.0.0-allinone  # Changed from v1.5.0
    # All other settings remain the same
```

### Breaking Changes
- None - API is fully backward compatible

### Recommended Actions
1. Pull new image: `docker pull neosun/orpheus-tts:v2.0.0-allinone`
2. Stop old container: `docker stop orpheus-tts`
3. Remove old container: `docker rm orpheus-tts`
4. Start new container with same configuration
5. Verify: `curl http://localhost:8899/health`

## ğŸ“¦ Docker Images

Available tags:
- `neosun/orpheus-tts:v2.0.0-allinone` (recommended)
- `neosun/orpheus-tts:v2.0.0-awq-4bit`
- `neosun/orpheus-tts:latest`

Image size: 11.1GB  
Digest: `sha256:686a55ef49a607bad0ba2bda472cb54cb5846af3609b2b8f2bfd2a251546f077`

## ğŸ› Known Issues

None reported.

## ğŸ™ Acknowledgments

- [Hariprasath28](https://huggingface.co/Hariprasath28) for the AWQ quantized model
- [Canopy Labs](https://canopylabs.ai/) for the original Orpheus TTS model
- [vLLM](https://github.com/vllm-project/vllm) for efficient AWQ inference support

## ğŸ“ Changelog

### Added
- AWQ 4-bit quantization support
- Model preloading on container startup
- Performance timing headers in API responses
- Version and model info in health endpoint

### Changed
- Model from canopylabs/orpheus-3b-0.1-ft to Hariprasath28/orpheus-3b-4bit-AWQ
- Dtype from bfloat16 to float16 (required for AWQ)
- Generation speed improved by 16-40%

### Fixed
- None

## ğŸ”— Links

- **Docker Hub**: https://hub.docker.com/r/neosun/orpheus-tts
- **GitHub**: https://github.com/neosun100/orpheus-tts-docker
- **Documentation**: https://github.com/neosun100/orpheus-tts-docker/blob/main/README.md
- **Original Model**: https://huggingface.co/canopylabs/orpheus-3b-0.1-ft
- **AWQ Model**: https://huggingface.co/Hariprasath28/orpheus-3b-4bit-AWQ

---

**Full Changelog**: https://github.com/neosun100/orpheus-tts-docker/compare/v1.5.0-allinone...v2.0.0-allinone
