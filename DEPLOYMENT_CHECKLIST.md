# ğŸš€ Orpheus TTS éƒ¨ç½²æ£€æŸ¥æ¸…å•

## ğŸ“‹ éƒ¨ç½²å‰æ£€æŸ¥

### ç³»ç»Ÿç¯å¢ƒ

- [ ] å·²å®‰è£… Docker
  ```bash
  docker --version
  ```

- [ ] å·²å®‰è£… Docker Compose
  ```bash
  docker-compose --version
  ```

- [ ] å·²å®‰è£… NVIDIA é©±åŠ¨
  ```bash
  nvidia-smi
  ```

- [ ] å·²å®‰è£… nvidia-docker
  ```bash
  docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
  ```

- [ ] GPU æ˜¾å­˜å……è¶³ï¼ˆè‡³å°‘ 8GBï¼‰
  ```bash
  nvidia-smi --query-gpu=memory.free --format=csv,noheader,nounits
  ```

### ç«¯å£æ£€æŸ¥

- [ ] ç«¯å£ 8899 æœªè¢«å ç”¨ï¼ˆæˆ–ä¿®æ”¹ `.env` ä¸­çš„ `PORT`ï¼‰
  ```bash
  ss -tuln | grep 8899
  ```

### æ–‡ä»¶æ£€æŸ¥

- [ ] æ‰€æœ‰å¿…éœ€æ–‡ä»¶å­˜åœ¨
  ```bash
  ls -l Dockerfile docker-compose.yml requirements.txt .env.example start.sh
  ```

- [ ] å¯åŠ¨è„šæœ¬æœ‰æ‰§è¡Œæƒé™
  ```bash
  ls -l start.sh test_deployment.sh
  ```

## ğŸš€ éƒ¨ç½²æ­¥éª¤

### 1. åˆ›å»ºç¯å¢ƒé…ç½®

- [ ] å¤åˆ¶ç¯å¢ƒå˜é‡æ–‡ä»¶
  ```bash
  cp .env.example .env
  ```

- [ ] æ ¹æ®éœ€è¦ä¿®æ”¹ `.env`
  ```bash
  nano .env
  # ä¿®æ”¹ PORT, GPU_IDLE_TIMEOUT ç­‰
  ```

### 2. å¯åŠ¨æœåŠ¡

- [ ] è¿è¡Œå¯åŠ¨è„šæœ¬
  ```bash
  ./start.sh
  ```

- [ ] ç­‰å¾…å®¹å™¨å¯åŠ¨å®Œæˆ
  ```bash
  docker-compose logs -f
  # çœ‹åˆ° "Running on http://0.0.0.0:8899" è¡¨ç¤ºæˆåŠŸ
  ```

### 3. éªŒè¯éƒ¨ç½²

- [ ] å¥åº·æ£€æŸ¥é€šè¿‡
  ```bash
  curl http://0.0.0.0:8899/health
  # åº”è¿”å› {"status": "ok", ...}
  ```

- [ ] UI å¯è®¿é—®
  ```bash
  curl -I http://0.0.0.0:8899/
  # åº”è¿”å› HTTP/1.1 200 OK
  ```

- [ ] API æ–‡æ¡£å¯è®¿é—®
  ```bash
  curl -I http://0.0.0.0:8899/apidocs
  # åº”è¿”å› HTTP/1.1 200 OK
  ```

### 4. åŠŸèƒ½æµ‹è¯•

- [ ] è¿è¡Œæµ‹è¯•è„šæœ¬
  ```bash
  ./test_deployment.sh
  ```

- [ ] æµ‹è¯• UI ç”Ÿæˆ
  - æ‰“å¼€æµè§ˆå™¨è®¿é—® http://0.0.0.0:8899
  - è¾“å…¥æµ‹è¯•æ–‡æœ¬
  - ç‚¹å‡»ç”Ÿæˆ
  - æ’­æ”¾éŸ³é¢‘

- [ ] æµ‹è¯• API ç”Ÿæˆ
  ```bash
  curl -X POST http://0.0.0.0:8899/api/generate \
    -H "Content-Type: application/json" \
    -d '{"text": "This is a test.", "voice": "tara"}' \
    --output test.wav
  
  # æ£€æŸ¥æ–‡ä»¶
  file test.wav
  # åº”æ˜¾ç¤º: RIFF (little-endian) data, WAVE audio
  ```

- [ ] æµ‹è¯• GPU çŠ¶æ€
  ```bash
  curl http://0.0.0.0:8899/health | jq '.gpu_status'
  ```

- [ ] æµ‹è¯• GPU é‡Šæ”¾
  ```bash
  curl -X POST http://0.0.0.0:8899/api/offload
  ```

## ğŸ¯ MCP éƒ¨ç½²ï¼ˆå¯é€‰ï¼‰

### 1. é…ç½® MCP æœåŠ¡å™¨

- [ ] å¤åˆ¶ MCP é…ç½®
  ```bash
  cp mcp_config.json ~/.config/mcp/config.json
  # æˆ–æ ¹æ®ä½ çš„ MCP å®¢æˆ·ç«¯é…ç½®è·¯å¾„
  ```

- [ ] ä¿®æ”¹é…ç½®ä¸­çš„è·¯å¾„
  ```bash
  nano ~/.config/mcp/config.json
  # ä¿®æ”¹ args ä¸­çš„è·¯å¾„ä¸ºå®é™…è·¯å¾„
  ```

### 2. æµ‹è¯• MCP

- [ ] å¯åŠ¨ MCP æœåŠ¡å™¨
  ```bash
  python3 mcp_server.py
  ```

- [ ] æµ‹è¯• MCP å·¥å…·ï¼ˆä½¿ç”¨ä½ çš„ MCP å®¢æˆ·ç«¯ï¼‰
  ```python
  # åˆ—å‡ºæ¨¡å‹
  result = await mcp.call_tool("list_models", {})
  
  # ç”Ÿæˆè¯­éŸ³
  result = await mcp.call_tool(
      "generate_speech",
      {"text": "Hello", "output_path": "/tmp/test.wav"}
  )
  ```

## ğŸ“Š ç›‘æ§æ£€æŸ¥

### å®¹å™¨çŠ¶æ€

- [ ] å®¹å™¨æ­£åœ¨è¿è¡Œ
  ```bash
  docker-compose ps
  # åº”æ˜¾ç¤º State: Up
  ```

- [ ] å®¹å™¨æ—¥å¿—æ­£å¸¸
  ```bash
  docker-compose logs --tail=50
  # æ— é”™è¯¯ä¿¡æ¯
  ```

### GPU çŠ¶æ€

- [ ] GPU è¢«å®¹å™¨ä½¿ç”¨
  ```bash
  nvidia-smi
  # åº”çœ‹åˆ° orpheus-tts è¿›ç¨‹
  ```

- [ ] æ˜¾å­˜å ç”¨æ­£å¸¸
  ```bash
  nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits
  # é¦–æ¬¡åŠ è½½ååº”è¯¥åœ¨ 8GB å·¦å³
  ```

### ç½‘ç»œçŠ¶æ€

- [ ] ç«¯å£æ­£åœ¨ç›‘å¬
  ```bash
  ss -tuln | grep 8899
  # åº”æ˜¾ç¤º LISTEN çŠ¶æ€
  ```

- [ ] å¯ä»å…¶ä»–æœºå™¨è®¿é—®ï¼ˆå¦‚æœéœ€è¦ï¼‰
  ```bash
  # ä»å…¶ä»–æœºå™¨
  curl http://<æœåŠ¡å™¨IP>:8899/health
  ```

## ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰

### é¢„åŠ è½½æ¨¡å‹

- [ ] ä¿®æ”¹ `server.py` æ·»åŠ é¢„åŠ è½½ä»£ç 
  ```python
  # åœ¨ if __name__ == '__main__' ä¹‹å‰
  for model_name in ["medium-3b"]:
      try:
          gpu_manager.get_model(model_name, lambda: load_model(model_name))
      except:
          pass
  ```

- [ ] é‡å¯æœåŠ¡
  ```bash
  docker-compose restart
  ```

### è°ƒæ•´è¶…æ—¶

- [ ] æ ¹æ®ä½¿ç”¨åœºæ™¯è°ƒæ•´ `GPU_IDLE_TIMEOUT`
  ```bash
  # å¼€å‘: 30 ç§’
  # ç”Ÿäº§: 600 ç§’
  nano .env
  docker-compose restart
  ```

## ğŸ” å®‰å…¨åŠ å›ºï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰

### åŸºç¡€å®‰å…¨

- [ ] ä¿®æ”¹é»˜è®¤ç«¯å£
- [ ] é…ç½®é˜²ç«å¢™è§„åˆ™
- [ ] é™åˆ¶è®¿é—® IP
- [ ] æ·»åŠ é€Ÿç‡é™åˆ¶

### é«˜çº§å®‰å…¨

- [ ] æ·»åŠ è®¤è¯ä¸­é—´ä»¶ï¼ˆJWT/API Keyï¼‰
- [ ] é…ç½® HTTPSï¼ˆNginx/Caddyï¼‰
- [ ] å¯ç”¨æ—¥å¿—å®¡è®¡
- [ ] è®¾ç½®èµ„æºé™åˆ¶

## ğŸ“ æ–‡æ¡£æ£€æŸ¥

- [ ] é˜…è¯» [QUICK_START.md](QUICK_START.md)
- [ ] é˜…è¯» [DOCKER_DEPLOYMENT.md](DOCKER_DEPLOYMENT.md)
- [ ] é˜…è¯» [MCP_GUIDE.md](MCP_GUIDE.md)ï¼ˆå¦‚æœä½¿ç”¨ MCPï¼‰
- [ ] é˜…è¯» [ARCHITECTURE.md](ARCHITECTURE.md)ï¼ˆäº†è§£æ¶æ„ï¼‰

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

- [ ] å¦‚æœç«¯å£è¢«å ç”¨ â†’ ä¿®æ”¹ `.env` ä¸­çš„ `PORT`
- [ ] å¦‚æœæ˜¾å­˜ä¸è¶³ â†’ è®¿é—® `/api/offload` é‡Šæ”¾æ˜¾å­˜
- [ ] å¦‚æœæ¨¡å‹ä¸‹è½½æ…¢ â†’ è®¾ç½® `HF_ENDPOINT=https://hf-mirror.com`
- [ ] å¦‚æœ GPU ä¸å¯ç”¨ â†’ æ£€æŸ¥ `nvidia-smi` å’Œ `nvidia-docker`
- [ ] å¦‚æœ vLLM æŠ¥é”™ â†’ å›é€€åˆ° `vllm==0.7.3`

### æ—¥å¿—æŸ¥çœ‹

- [ ] æŸ¥çœ‹å®¹å™¨æ—¥å¿—
  ```bash
  docker-compose logs -f
  ```

- [ ] æŸ¥çœ‹ GPU æ—¥å¿—
  ```bash
  nvidia-smi dmon
  ```

## âœ… éƒ¨ç½²å®Œæˆ

æ­å–œï¼å¦‚æœæ‰€æœ‰æ£€æŸ¥é¡¹éƒ½é€šè¿‡ï¼Œä½ çš„ Orpheus TTS æœåŠ¡å·²æˆåŠŸéƒ¨ç½²ï¼

### è®¿é—®ä¿¡æ¯

- **UI ç•Œé¢**: http://0.0.0.0:8899
- **API æ–‡æ¡£**: http://0.0.0.0:8899/apidocs
- **å¥åº·æ£€æŸ¥**: http://0.0.0.0:8899/health

### å¸¸ç”¨å‘½ä»¤

```bash
# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f

# åœæ­¢æœåŠ¡
docker-compose down

# é‡å¯æœåŠ¡
docker-compose restart

# æŸ¥çœ‹ GPU
nvidia-smi

# æµ‹è¯•éƒ¨ç½²
./test_deployment.sh
```

### ä¸‹ä¸€æ­¥

- [ ] é›†æˆåˆ°ä½ çš„åº”ç”¨
- [ ] é…ç½®ç›‘æ§å’Œå‘Šè­¦
- [ ] è®¾ç½®è‡ªåŠ¨å¤‡ä»½
- [ ] ä¼˜åŒ–æ€§èƒ½å‚æ•°
- [ ] æ·»åŠ å®‰å…¨æªæ–½

---

**éœ€è¦å¸®åŠ©ï¼Ÿ**
- æŸ¥çœ‹æ–‡æ¡£: [DOCKER_DEPLOYMENT.md](DOCKER_DEPLOYMENT.md)
- æäº¤ Issue: [GitHub Issues](https://github.com/canopyai/Orpheus-TTS/issues)
